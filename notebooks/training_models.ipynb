{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Josh Barrios 09/09/2020\n",
    "\n",
    "Training DeepBLU-Z models\n",
    "\n",
    "##% md\n",
    "Train resnet18 model on zebrafish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled\r\n",
      "2020-09-09 13:16:23,178:INFO: Start with arguments Namespace(backbone='resnet18', batch_size=16, datapath=PosixPath('../data/training_data'), epochs=2, load='./models/082020/resnet18', lr=0.01, lr_decay=0.001, lr_decay_step=5, mode='train', num_pts=8, retrain=False, save='../models/090920/resnet18_2epochs', seed=423394486, shuffle=True, target='./data/test_data/image8.tif', transform=False, val_split=0.2)\r\n",
      "2020-09-09 13:16:24,725:INFO: Building new model for training\r\n",
      "2020-09-09 13:16:24,726:INFO: Model:\r\n",
      "Model(\r\n",
      "  (features): ResNet(\r\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\r\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "    (relu): ReLU(inplace=True)\r\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\r\n",
      "    (layer1): Sequential(\r\n",
      "      (0): BasicBlock(\r\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "      )\r\n",
      "      (1): BasicBlock(\r\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer2): Sequential(\r\n",
      "      (0): BasicBlock(\r\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): BasicBlock(\r\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer3): Sequential(\r\n",
      "      (0): BasicBlock(\r\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): BasicBlock(\r\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer4): Sequential(\r\n",
      "      (0): BasicBlock(\r\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): BasicBlock(\r\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (relu): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\r\n",
      "    (fc): Linear(in_features=512, out_features=16, bias=True)\r\n",
      "  )\r\n",
      ")\r\n",
      "2020-09-09 13:16:28,069:INFO: Train: epoch 0   learning rate: 0.01\r\n",
      "2020-09-09 13:16:30,051:INFO: epoch 1/2, step 5/2992,  loss 1.3131486218068211\r\n",
      "2020-09-09 13:16:31,983:INFO: epoch 1/2, step 10/2992,  loss 0.23779123425400425\r\n",
      "2020-09-09 13:16:33,918:INFO: epoch 1/2, step 15/2992,  loss 0.087380587005125\r\n",
      "2020-09-09 13:16:35,846:INFO: epoch 1/2, step 20/2992,  loss 0.07817178784004129\r\n",
      "2020-09-09 13:16:37,784:INFO: epoch 1/2, step 25/2992,  loss 0.027536272797916244\r\n",
      "2020-09-09 13:16:39,716:INFO: epoch 1/2, step 30/2992,  loss 0.02595748486314279\r\n",
      "2020-09-09 13:16:41,649:INFO: epoch 1/2, step 35/2992,  loss 0.008943359763793714\r\n",
      "2020-09-09 13:16:43,603:INFO: epoch 1/2, step 40/2992,  loss 0.017714675714236298\r\n",
      "2020-09-09 13:16:45,560:INFO: epoch 1/2, step 45/2992,  loss 0.008049110212288199\r\n",
      "2020-09-09 13:16:47,548:INFO: epoch 1/2, step 50/2992,  loss 0.01485197638997382\r\n",
      "2020-09-09 13:16:49,516:INFO: epoch 1/2, step 55/2992,  loss 0.010460894232539479\r\n",
      "2020-09-09 13:16:51,477:INFO: epoch 1/2, step 60/2992,  loss 0.017359093759996256\r\n",
      "2020-09-09 13:16:53,444:INFO: epoch 1/2, step 65/2992,  loss 0.006700156761530591\r\n",
      "2020-09-09 13:16:55,415:INFO: epoch 1/2, step 70/2992,  loss 0.009401903747455568\r\n",
      "2020-09-09 13:16:57,378:INFO: epoch 1/2, step 75/2992,  loss 0.010180273222460514\r\n",
      "2020-09-09 13:16:59,335:INFO: epoch 1/2, step 80/2992,  loss 0.004477459532264463\r\n",
      "2020-09-09 13:17:01,298:INFO: epoch 1/2, step 85/2992,  loss 0.011472617803679778\r\n",
      "2020-09-09 13:17:03,371:INFO: epoch 1/2, step 90/2992,  loss 0.011701178718992613\r\n",
      "2020-09-09 13:17:05,364:INFO: epoch 1/2, step 95/2992,  loss 0.00605721430566725\r\n",
      "2020-09-09 13:17:07,327:INFO: epoch 1/2, step 100/2992,  loss 0.0036134705860840707\r\n",
      "2020-09-09 13:17:09,274:INFO: epoch 1/2, step 105/2992,  loss 0.006503416744099374\r\n",
      "2020-09-09 13:17:11,237:INFO: epoch 1/2, step 110/2992,  loss 0.0038378087018218613\r\n",
      "2020-09-09 13:17:13,193:INFO: epoch 1/2, step 115/2992,  loss 0.008529622000555405\r\n",
      "2020-09-09 13:17:15,144:INFO: epoch 1/2, step 120/2992,  loss 0.012342867092327268\r\n",
      "2020-09-09 13:17:17,101:INFO: epoch 1/2, step 125/2992,  loss 0.009125303144813192\r\n",
      "2020-09-09 13:17:19,068:INFO: epoch 1/2, step 130/2992,  loss 0.00541906619004384\r\n",
      "2020-09-09 13:17:21,046:INFO: epoch 1/2, step 135/2992,  loss 0.004709072670859972\r\n",
      "2020-09-09 13:17:23,036:INFO: epoch 1/2, step 140/2992,  loss 0.007155239582674616\r\n",
      "2020-09-09 13:17:25,043:INFO: epoch 1/2, step 145/2992,  loss 0.00677851669646051\r\n",
      "2020-09-09 13:17:26,992:INFO: epoch 1/2, step 150/2992,  loss 0.007592160837875262\r\n",
      "2020-09-09 13:17:28,974:INFO: epoch 1/2, step 155/2992,  loss 0.008623495769707787\r\n",
      "2020-09-09 13:17:30,925:INFO: epoch 1/2, step 160/2992,  loss 0.007360366643892169\r\n",
      "2020-09-09 13:17:32,888:INFO: epoch 1/2, step 165/2992,  loss 0.011101099493656195\r\n",
      "2020-09-09 13:17:34,843:INFO: epoch 1/2, step 170/2992,  loss 0.0029055218428974145\r\n",
      "2020-09-09 13:17:36,812:INFO: epoch 1/2, step 175/2992,  loss 0.00700886842017457\r\n",
      "2020-09-09 13:17:38,793:INFO: epoch 1/2, step 180/2992,  loss 0.004780124504173477\r\n",
      "2020-09-09 13:17:40,780:INFO: epoch 1/2, step 185/2992,  loss 0.005712431883617692\r\n",
      "2020-09-09 13:17:42,746:INFO: epoch 1/2, step 190/2992,  loss 0.0036852259103900714\r\n",
      "2020-09-09 13:17:44,766:INFO: epoch 1/2, step 195/2992,  loss 0.01318542271625579\r\n",
      "2020-09-09 13:17:46,798:INFO: epoch 1/2, step 200/2992,  loss 0.006775639253653629\r\n",
      "2020-09-09 13:17:48,761:INFO: epoch 1/2, step 205/2992,  loss 0.00739208629608889\r\n",
      "2020-09-09 13:17:50,725:INFO: epoch 1/2, step 210/2992,  loss 0.0030502319802466525\r\n",
      "2020-09-09 13:17:52,674:INFO: epoch 1/2, step 215/2992,  loss 0.0031121483403108267\r\n",
      "2020-09-09 13:17:54,636:INFO: epoch 1/2, step 220/2992,  loss 0.004846128643943284\r\n",
      "2020-09-09 13:17:56,590:INFO: epoch 1/2, step 225/2992,  loss 0.0051150485166112074\r\n",
      "2020-09-09 13:17:58,548:INFO: epoch 1/2, step 230/2992,  loss 0.005859310058035106\r\n",
      "2020-09-09 13:18:00,505:INFO: epoch 1/2, step 235/2992,  loss 0.005006511589887085\r\n",
      "2020-09-09 13:18:02,462:INFO: epoch 1/2, step 240/2992,  loss 0.005793812700363856\r\n",
      "2020-09-09 13:18:04,414:INFO: epoch 1/2, step 245/2992,  loss 0.00468353442577665\r\n",
      "2020-09-09 13:18:06,368:INFO: epoch 1/2, step 250/2992,  loss 0.004232297980388582\r\n",
      "2020-09-09 13:18:08,333:INFO: epoch 1/2, step 255/2992,  loss 0.00599067581837065\r\n",
      "2020-09-09 13:18:10,288:INFO: epoch 1/2, step 260/2992,  loss 0.006228185577925528\r\n",
      "2020-09-09 13:18:12,248:INFO: epoch 1/2, step 265/2992,  loss 0.005978090036342081\r\n",
      "2020-09-09 13:18:14,263:INFO: epoch 1/2, step 270/2992,  loss 0.0044940428189819285\r\n",
      "2020-09-09 13:18:16,223:INFO: epoch 1/2, step 275/2992,  loss 0.002296570380267601\r\n",
      "2020-09-09 13:18:18,245:INFO: epoch 1/2, step 280/2992,  loss 0.004274698063398464\r\n",
      "2020-09-09 13:18:20,240:INFO: epoch 1/2, step 285/2992,  loss 0.005346200586989541\r\n",
      "2020-09-09 13:18:22,224:INFO: epoch 1/2, step 290/2992,  loss 0.010277069767444339\r\n"
     ]
    }
   ],
   "source": [
    "! python3 ../main.py -m train --save ../models/090920/resnet18_2epochs --datapath ../data/training_data -e 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train resnet18 model on Danionella data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled\r\n",
      "2020-09-09 12:58:27,268:INFO: Start with arguments Namespace(backbone='resnet18', batch_size=16, datapath=PosixPath('../data/danionella_training_data'), epochs=50, load='../models/082820/resnet18_50epochs', lr=0.01, lr_decay=0.001, lr_decay_step=5, mode='train', num_pts=8, retrain=True, save='../models/090920/resnet18_50epochs_danionella', seed=995184371, shuffle=True, target='./data/test_data/image8.tif', transform=False, val_split=0.2)\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"../main.py\", line 373, in <module>\r\n",
      "    main(args)\r\n",
      "  File \"../main.py\", line 342, in main\r\n",
      "    model.load_state_dict(torch.load(args.load))\r\n",
      "  File \"/home/userman/.virtualenvs/DeepBLU-Z/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 847, in load_state_dict\r\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n",
      "RuntimeError: Error(s) in loading state_dict for Model:\r\n",
      "\tsize mismatch for features.conv1.weight: copying a param with shape torch.Size([64, 1, 7, 7]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\r\n"
     ]
    }
   ],
   "source": [
    "! python3 ../main.py -m train --save ../models/090920/resnet18_50epochs_danionella --datapath ../data/danionella_training_data --load ../models/082820/resnet18_50epochs  --retrain true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}